
## 核心论点：浪费是程序变慢的最大元凶

文章指出，提升程序性能只有两种方法：**减少CPU需要执行的指令总数**，或者**优化指令使其在CPU中执行得更快**。本文的核心观点是，导致程序缓慢成千上万倍的最主要因素是 **“浪费” (Waste)**。

这里的“浪费”指的是CPU执行了大量与用户实际编程意图无关的、完全不必要的指令。

-----

## C语言 vs. Python：一个简单的加法例子

为了说明“浪费”有多严重，文章对比了C语言和Python执行一个简单的加法操作 `A + B` 的情况。

  * **在C语言中**：

      * 一个简单的 `A + B` 操作被编译器高效地翻译成**一条**CPU指令（例如 `LEA`）。
      * 这代表了理想情况，几乎**没有浪费**。

  * **在Python中**：

      * 同样是 `A + B` 操作，由于Python是解释型语言，它不会直接生成CPU指令。
      * Python解释器为了解码和执行这一行代码，最终向CPU发送了大约 **181条** 指令。
      * 在这181条指令中，只有**一条**是真正执行加法操作的，其余的180条都是解释器自身的开销，即“浪费”。



-----

## 性能实测对比

文章通过一个累加4096个整数的循环任务，对两种语言进行了性能测试，结果进一步证实了上述观点。

  * **C语言程序**：每个时钟周期大约能完成 **0.8** 次加法运算。
  * **Python程序**：每个时钟周期大约只能完成 **0.006** 次加法运算。

结论是，在这个任务上，Python程序比一个**未经过优化的C程序**慢了大约 **130倍**。作者强调，如果C程序开启优化，这个差距会更大。
